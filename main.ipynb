{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BEVFormer++ Project\n",
                "This notebook implements and verifies the enhanced BEVFormer model with Memory Bank and ConvRNN.\n",
                "\n",
                "## Data Setup\n",
                "To use real data, please download the **nuScenes mini** dataset:\n",
                "1. Register/Login at [nuscenes.org](https://www.nuscenes.org/nuscenes#download)\n",
                "2. Download the \"mini\" split.\n",
                "3. Extract it to `./data/nuscenes`.\n",
                "   Structure should be:\n",
                "   ```\n",
                "   data/\n",
                "     nuscenes/\n",
                "       maps/\n",
                "       samples/\n",
                "       sweeps/\n",
                "       v1.0-mini/\n",
                "   ```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import cv2\n",
                "import os\n",
                "import sys\n",
                "import importlib\n",
                "\n",
                "# Add current directory to path\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "from modules.dataset import CarlaDataset, NuScenesDataset\n",
                "import modules.bevformer\n",
                "# Force reload to ensure latest changes are picked up\n",
                "importlib.reload(modules.bevformer)\n",
                "from modules.bevformer import EnhancedBEVFormer\n",
                "\n",
                "# Check device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data (Dummy or Real)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "USE_REAL_DATA = False # Set to True if you want to visualize Real Data\n",
                "\n",
                "if USE_REAL_DATA and os.path.exists('data/v1.0-mini'):\n",
                "    print(\"Loading NuScenes Mini dataset...\")\n",
                "    dataset = NuScenesDataset(version='v1.0-mini', dataroot='data')\n",
                "else:\n",
                "    print(\"Using Dummy Data (NuScenes not found or disabled)...\")\n",
                "    dataset = CarlaDataset(root_dir='data', dummy_mode=True)\n",
                "\n",
                "data = dataset[0]\n",
                "\n",
                "imgs = data['img'].to(device) # (Seq, 6, 3, H, W)\n",
                "intrinsics = data['intrinsics'].to(device)\n",
                "extrinsics = data['extrinsics'].to(device)\n",
                "ego_pose = data['ego_pose'].to(device)\n",
                "\n",
                "print(f\"Images shape: {imgs.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialize Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = EnhancedBEVFormer(bev_h=200, bev_w=200, embed_dim=256).to(device)\n",
                "\n",
                "# Load Checkpoint if available\n",
                "checkpoint_path = 'checkpoints/latest.pth'\n",
                "if os.path.exists(checkpoint_path):\n",
                "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
                "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
                "    model.load_state_dict(checkpoint['backbone_state_dict'], strict=False)\n",
                "    print(\"Checkpoint loaded successfully.\")\n",
                "else:\n",
                "    print(\"No checkpoint found. Using random initialization.\")\n",
                "\n",
                "print(\"Model initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Forward Pass Verification (Sequence)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run forward pass on the sequence\n",
                "with torch.no_grad():\n",
                "    # imgs: (Seq, 6, 3, H, W) -> (1, Seq, 6, 3, H, W)\n",
                "    seq_imgs = imgs.unsqueeze(0)\n",
                "    seq_intrinsics = intrinsics.unsqueeze(0)\n",
                "    seq_extrinsics = extrinsics.unsqueeze(0)\n",
                "    seq_ego_pose = ego_pose.unsqueeze(0)\n",
                "    \n",
                "    # Use forward_sequence\n",
                "    if hasattr(model, 'forward_sequence'):\n",
                "        bev_seq_output = model.forward_sequence(seq_imgs, seq_intrinsics, seq_extrinsics, seq_ego_pose)\n",
                "        # Take the last frame output\n",
                "        bev_output = bev_seq_output[:, -1]\n",
                "        print(f\"BEV Sequence Output shape: {bev_seq_output.shape}\")\n",
                "        print(f\"Last Frame BEV Output shape: {bev_output.shape}\")\n",
                "    else:\n",
                "        print(\"ERROR: forward_sequence method missing despite reload!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the feature map (average across channels)\n",
                "if 'bev_output' in locals():\n",
                "    bev_map = bev_output[0].mean(dim=0).cpu().numpy()\n",
                "\n",
                "    plt.figure(figsize=(10, 10))\n",
                "    plt.imshow(bev_map, cmap='viridis')\n",
                "    plt.title(\"BEV Feature Map (Last Frame)\")\n",
                "    plt.colorbar()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interpretability: Spatial Correspondence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def project_bev_to_cam(bev_pos, intrinsic, extrinsic, img_shape):\n",
                "    # bev_pos: (H, W, 3)\n",
                "    # intrinsic: (3, 3)\n",
                "    # extrinsic: (4, 4)\n",
                "    H, W, _ = bev_pos.shape\n",
                "    pts_3d = bev_pos.reshape(-1, 3)\n",
                "    ones = torch.ones(pts_3d.shape[0], 1, device=pts_3d.device)\n",
                "    pts_hom = torch.cat([pts_3d, ones], dim=1) # (N, 4)\n",
                "    \n",
                "    # Transform to camera\n",
                "    cam_coords = (pts_hom @ extrinsic.T)[:, :3] # (N, 3)\n",
                "    \n",
                "    # Project\n",
                "    img_coords = (cam_coords @ intrinsic.T) # (N, 3)\n",
                "    u = img_coords[:, 0] / (img_coords[:, 2] + 1e-5)\n",
                "    v = img_coords[:, 1] / (img_coords[:, 2] + 1e-5)\n",
                "    z = img_coords[:, 2]\n",
                "    \n",
                "    mask = (z > 0) & (u >= 0) & (u < img_shape[1]) & (v >= 0) & (v < img_shape[0])\n",
                "    return u[mask], v[mask]\n",
                "\n",
                "# Get BEV grid from model\n",
                "bev_pos = model.bev_pos[0] # (H, W, 3)\n",
                "\n",
                "# Visualize on the first camera\n",
                "cam_idx = 0\n",
                "img_tensor = imgs[-1, cam_idx].cpu().permute(1, 2, 0).numpy() # Use last frame image\n",
                "img_tensor = (img_tensor - img_tensor.min()) / (img_tensor.max() - img_tensor.min()) # Normalize for display\n",
                "\n",
                "intrinsic = intrinsics[-1, cam_idx]\n",
                "extrinsic = extrinsics[-1, cam_idx]\n",
                "\n",
                "u, v = project_bev_to_cam(bev_pos, intrinsic, extrinsic, (256, 704))\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.imshow(img_tensor)\n",
                "plt.scatter(u.cpu().numpy(), v.cpu().numpy(), s=1, c='red', alpha=0.5)\n",
                "plt.title(f\"BEV Grid Projection on Camera {cam_idx}\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
